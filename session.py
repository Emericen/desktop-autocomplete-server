import os

import openai
from pydantic import BaseModel, Field

VLLM_BASE_URL = os.getenv("VLLM_BASE_URL", "http://localhost:8000/v1")
MODEL = os.getenv("MODEL", "Qwen/Qwen3-VL-32B-Instruct-FP8")
API_KEY = os.getenv("API_KEY", "EMPTY")
MAX_PREDICT_TOKENS = int(os.getenv("MAX_PREDICT_TOKENS", "20"))
MAX_COMPACT_TOKENS = int(os.getenv("MAX_COMPACT_TOKENS", "300"))
TEMPERATURE = float(os.getenv("TEMPERATURE", "0.0"))
MAX_MODEL_LEN = int(os.getenv("MAX_MODEL_LEN", "20000"))


class Action(BaseModel):
    id: str
    timestamp: int

    def to_llm_content_blocks(self) -> list[dict]:
        raise NotImplementedError("Subclasses must implement this method")


class TypingAction(Action):
    text: str
    screenshot: str

    def to_llm_content_blocks(self) -> list[dict]:
        return [
            {"type": "text", "text": f'Typed: "{self.text}"'},
            {"type": "image_url", "image_url": {"url": self.screenshot}},
        ]


class MouseClickAction(Action):
    button: str
    x: float
    y: float
    screenshot: str

    def to_llm_content_blocks(self) -> list[dict]:
        return [
            {"type": "text", "text": f"{self.button.capitalize()} click"},
            {"type": "image_url", "image_url": {"url": self.screenshot}},
        ]


class MouseDragAction(Action):
    button: str
    start_x: float = Field(alias="startX")
    start_y: float = Field(alias="startY")
    end_x: float = Field(alias="endX")
    end_y: float = Field(alias="endY")
    screenshot: str

    def to_llm_content_blocks(self) -> list[dict]:
        return [
            {"type": "text", "text": f"Dragged with {self.button} mouse button"},
            {"type": "image_url", "image_url": {"url": self.screenshot}},
        ]


class ScrollAction(Action):
    x: float
    y: float
    screenshot: str

    def to_llm_content_blocks(self) -> list[dict]:
        return [
            {"type": "text", "text": "Scrolled"},
            {"type": "image_url", "image_url": {"url": self.screenshot}},
        ]


class HotkeyAction(Action):
    modifiers: list[str]
    key: str

    def to_llm_content_blocks(self) -> list[dict]:
        combo = " + ".join(m.capitalize() for m in self.modifiers) + f" + {self.key}"
        return [{"type": "text", "text": f"Hit `{combo}`"}]


class SpecialKeyAction(Action):
    key: str

    def to_llm_content_blocks(self) -> list[dict]:
        return [{"type": "text", "text": f"Hit `{self.key}`"}]


class AutocompleteAction(Action):
    text: str

    def to_llm_content_blocks(self) -> list[dict]:
        return [{"type": "text", "text": f'Accepted: "{self.text}"'}]


def parse_action(data: dict) -> Action:
    action_type = data.get("type")
    if action_type == "typing":
        return TypingAction(**data)
    elif action_type == "mouse_click":
        return MouseClickAction(**data)
    elif action_type == "mouse_drag":
        return MouseDragAction(**data)
    elif action_type == "scroll":
        return ScrollAction(**data)
    elif action_type == "hotkey":
        return HotkeyAction(**data)
    elif action_type == "special_key":
        return SpecialKeyAction(**data)
    elif action_type == "autocomplete":
        return AutocompleteAction(**data)
    else:
        raise ValueError(f"Unknown action type: {action_type}")


SYSTEM_PROMPT = """
You are a desktop autocomplete assistant that watches over the user's computer and predicts what they are about to enter next. You will learn about the user over time, and your suggestion will be surfaced to the user as on-screen overlay bubble in which they can accept or deny. Should they accept, we will then type the text on their computer automatically. Your goal is to help the user spend less time typing and editing text.

You will receive timestamped events showing what the user did (typed text, mouse clicks, key presses, scrolling) paired with screenshots of what was on screen at that moment for each non-typing event.

We will also highlight user's cursor on the screenshots to help you identify the user's intent. 

A screenshot for a mouse click event will have a yellow ring around the cursor, indicating the click's location. For your info, the ring is generated by the following code:

```javascript
  drawClickRing(x, y, radius = 55, ringWidth = 10, alpha = 0.85) {
    const ctx = this.canvasContext

    // Draw outer glow (larger, blurred ring)
    ctx.beginPath()
    ctx.arc(x, y, radius + 5, 0, 2 * Math.PI)
    ctx.strokeStyle = `rgba(255, 235, 0, 0.3)`
    ctx.lineWidth = ringWidth + 15
    ctx.stroke()

    // Draw main ring
    ctx.beginPath()
    ctx.arc(x, y, radius, 0, 2 * Math.PI)
    ctx.strokeStyle = `rgba(255, 235, 0, ${alpha})`
    ctx.lineWidth = ringWidth
    ctx.stroke()
  }
```

Pay close attention to the user's cursor and the location of the click. Specifically, you should predict user's text when they:

#1. Clicked on an editable text field
#2. Have selected an editable text field and their latest event indicate they typed some text.

In case #1, you should predict the full extent of the text that the user is about to type. In case #2, you should predict the rest of the text that the user is about to type. (e.g. if their email is "john.doe@example.com", and they have typed "joh", you should predict "n.doe@example.com".)

Otherwise, you should skip the prediction and wait for the next event.

Respond with only your prediction text, or <skip> if you decide to skip the prediction.

""".strip()
COMPACT_PROMPT = """Summarize what happened here and everything you have seen so far into one paragraph of plain english text. Keep it short and concise."""


class Session:
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.client = openai.OpenAI(api_key=API_KEY, base_url=VLLM_BASE_URL)
        self.actions: list[Action] = []
        self.content_blocks: list[dict] = []
        self.context_length = 0

    def predict(self, action: dict) -> str:
        action = parse_action(action)
        self.content_blocks.extend(action.to_llm_content_blocks())
        self.actions.append(action)

        try:
            response = self.client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": self.content_blocks},
                ],
                max_tokens=MAX_PREDICT_TOKENS,
                temperature=TEMPERATURE,
            )
        except openai.BadRequestError as e:
            if "context length" in str(e).lower() or "too large" in str(e).lower():
                response = self.client.chat.completions.create(
                    model=MODEL,
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": self.content_blocks},
                        {"role": "user", "content": COMPACT_PROMPT},
                    ],
                    max_tokens=MAX_COMPACT_TOKENS,
                    temperature=TEMPERATURE,
                )
                summary = response.choices[0].message.content
                self.content_blocks = [{"type": "text", "text": summary}]
                response = self.client.chat.completions.create(
                    model=MODEL,
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": self.content_blocks},
                    ],
                    max_tokens=MAX_PREDICT_TOKENS,
                    temperature=TEMPERATURE,
                )
            else:
                raise

        self.context_length = response.usage.prompt_tokens
        return response.choices[0].message.content

    def clear(self):
        self.actions = []
        self.content_blocks = []
        self.context_length = 0
