services:
  vllm:
    image: vllm/vllm-openai:latest
    ipc: host
    shm_size: 16g
    restart: unless-stopped
    volumes:
      - ${HF_CACHE:-~/.cache/huggingface}:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command:
      - --model=${MODEL:-Qwen/Qwen3-VL-32B-Instruct-FP8}
      - --host=0.0.0.0
      - --port=${VLLM_PORT:-8000}
      - --served-model-name=${MODEL:-Qwen/Qwen3-VL-32B-Instruct-FP8}
      - --trust-remote-code
      - --dtype=${DTYPE:-auto}
      - --tensor-parallel-size=${TENSOR_PARALLEL:-1}
      - --enable-prefix-caching
      - --enable-auto-tool-choice
      - --tool-call-parser=${TOOL_CALL_PARSER:-hermes}
      - --limit-mm-per-prompt={"image":${MAX_IMAGE_COUNT:-100}}
      - --max-model-len=${MAX_MODEL_LEN:-100000}
      - --gpu-memory-utilization=${GPU_MEMORY_UTILIZATION:-0.95}
      - --max-num-seqs=${MAX_NUM_SEQS:-128}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_PORT:-8000}/health"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 600s

  api:
    build: .
    restart: unless-stopped
    ports:
      - target: 8080
        published: ${API_PORT:-8080}
        protocol: tcp
    volumes:
      - ./app.py:/app/app.py:ro
    environment:
      - VLLM_BASE_URL=http://vllm:${VLLM_PORT:-8000}/v1
      - MODEL=${MODEL:-Qwen/Qwen3-VL-32B-Instruct-FP8}
      - MAX_PREDICT_TOKENS=${MAX_PREDICT_TOKENS:-20}
      - MAX_COMPACT_TOKENS=${MAX_COMPACT_TOKENS:-200}
      - TEMPERATURE=${TEMPERATURE:-0.0}
      - MAX_MODEL_LEN=${MAX_MODEL_LEN:-100000}
    command: uvicorn app:app --host 0.0.0.0 --port 8080 --reload
    depends_on:
      vllm:
        condition: service_healthy
