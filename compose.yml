services:
  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    ipc: host
    shm_size: 16g
    restart: unless-stopped
    volumes:
      - ${HF_CACHE:-~/.cache/huggingface}:/root/.cache/huggingface
    environment:
      - NVIDIA_VISIBLE_DEVICES=${GPUS:-all}
    command:
      - --model=${MODEL:-Qwen/Qwen3-VL-32B-Instruct-FP8}
      - --host=0.0.0.0
      - --port=${VLLM_PORT:-8000}
      - --served-model-name=${MODEL:-Qwen/Qwen3-VL-32B-Instruct-FP8}
      - --trust-remote-code
      - --dtype=${DTYPE:-auto}
      - --tensor-parallel-size=${TENSOR_PARALLEL:-1}
      - --enable-prefix-caching
      - --enable-auto-tool-choice
      - --tool-call-parser=${TOOL_CALL_PARSER:-hermes}
      - --limit-mm-per-prompt={"image":${MAX_IMAGE_COUNT:-100}}
      - --max-model-len=${MAX_MODEL_LEN:-100000}
      - --gpu-memory-utilization=${GPU_MEMORY_UTILIZATION:-0.95}
      - --max-num-seqs=${MAX_NUM_SEQS:-128}
      - --prefix-caching-hash-algo=${PREFIX_CACHING_HASH_ALGO:-xxhash}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_PORT:-8000}/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  api:
    build: .
    restart: unless-stopped
    ports:
      - "${API_PORT:-8080}:8080"
    environment:
      - VLLM_BASE_URL=http://vllm:${VLLM_PORT:-8000}/v1
      - MODEL=${MODEL:-Qwen/Qwen3-VL-32B-Instruct-FP8}
      - MAX_PREDICT_TOKENS=${MAX_PREDICT_TOKENS:-20}
      - MAX_COMPACT_TOKENS=${MAX_COMPACT_TOKENS:-200}
      - TEMPERATURE=${TEMPERATURE:-0.0}
      - MAX_MODEL_LEN=${MAX_MODEL_LEN:-100000}
    depends_on:
      vllm:
        condition: service_healthy
